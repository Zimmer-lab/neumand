{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Diagnostics: Variability of the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "import helper_functions as hf\n",
    "import plotly.graph_objects as go\n",
    "from sklearn.metrics.cluster import contingency_matrix\n",
    "from munkres import Munkres\n",
    "import seaborn as sns\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn import metrics\n",
    "sns.set_theme()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_embedded = pd.read_hdf('time_embedded_2103.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=3)\n",
    "pca_data = pca.fit_transform(time_embedded.loc[:,~time_embedded.columns.isin(['state', 'dataset'])])\n",
    "data = pd.DataFrame(pca_data)\n",
    "data['state'] = time_embedded['state']\n",
    "data['dataset'] = time_embedded['dataset']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = {dataset: df for dataset, df in data.groupby('dataset')}\n",
    "hf.plot_PCs_separately(datasets).run_server(debug=True, port=8054)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Color-code trajectories based on dataset \n",
    "Each data point is colored based on the dataset it belongs to. This helps in understanding the variability of the data across different datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_traces = []\n",
    "\n",
    "for name, df in datasets.items():\n",
    "    trace = go.Scatter3d(x=df[0], y=df[1], z=df[2], mode=\"lines\", name=name)\n",
    "    all_traces.append(trace)\n",
    "    \n",
    "fig = go.Figure(data=all_traces)\n",
    "\n",
    "variances = pca.explained_variance_ratio_ * 100\n",
    "scene = dict(xaxis_title=f\"PC 1 ({variances[0]:.2f}%)\",\n",
    "                yaxis_title=f\"PC 2 ({variances[1]:.2f}%)\",\n",
    "                zaxis_title=f\"PC 3 ({variances[2]:.2f}%)\")\n",
    "\n",
    "fig.update_layout(scene=scene)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering with KMeans \n",
    "We can try clustering our data without the dataset feature to see if the separation of data points is based on the dataset membership or not.\n",
    "\n",
    "We could cross check with a dataset where no preprocessing has been done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "kmeans = KMeans(n_clusters=23)\n",
    "data['cluster'] = kmeans.fit_predict(time_embedded.loc[:,~time_embedded.columns.isin(['state', 'dataset'])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "data_copy = data.copy()\n",
    "# Assuming 'text_column' is your pandas DataFrame column with text values\n",
    "label_encoder = LabelEncoder()\n",
    "data_copy['dataset_numeric'] = label_encoder.fit_transform(data_copy['dataset'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_copy.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluation** <br>\n",
    "We can evaluate the clustering using the adjusted mutual information score, which calculates the mutual information between two clusterings and then normalizes this value by the expected mutual information of two random clusterings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.cluster import adjusted_mutual_info_score\n",
    "\n",
    "labels_pred_proc = data_copy['cluster']\n",
    "labels_true_proc = data_copy['dataset_numeric']\n",
    "\n",
    "# Calculate adjusted mutual information score, which tells us how well the clustering results match the ground truth\n",
    "print(adjusted_mutual_info_score(labels_true_proc, labels_pred_proc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Control Dataset\n",
    "We will now cluster our unpreprocessed data and see if the clusters are based on the dataset membership or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputed_dataframe = pd.read_hdf(\"imputed_dataframe_0602.h5\", key=\"data\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans2 = KMeans(n_clusters=23)\n",
    "imputed_dataframe['cluster'] = kmeans2.fit_predict(imputed_dataframe.loc[:,~imputed_dataframe.columns.isin(['state', 'dataset'])])\n",
    "labels_pred_unproc = imputed_dataframe['cluster']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputed_dataframe_copy = imputed_dataframe.copy()\n",
    "# Assuming 'text_column' is your pandas DataFrame column with text values\n",
    "label_encoder2 = LabelEncoder()\n",
    "imputed_dataframe_copy['dataset_numeric'] = label_encoder2.fit_transform(imputed_dataframe_copy['dataset'])\n",
    "labels_true_unproc = imputed_dataframe_copy['dataset_numeric']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(adjusted_mutual_info_score(imputed_dataframe_copy['cluster'], imputed_dataframe_copy['dataset_numeric']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Result**<br>\n",
    "Since the adjusted mutual information score between the clustering of our data and the dataset membership is relatively low (also compared to the unpreprocessed data), we can assume that the separation of data points is not based on the dataset membership and that the dataset feature might not explain the variability of the trajectories.\n",
    "\n",
    "\n",
    "#### Cluster Label Correspondence\n",
    "We want to match the predicted cluster labels to the dataset labels to see if the clustering is consistent with the dataset membership.\n",
    "\n",
    "**Preprocessed Data**<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = Munkres()\n",
    "contmat = contingency_matrix(labels_true_proc, labels_pred_proc)\n",
    "mapping = pd.DataFrame(m.compute(contmat.max() - contmat), columns=['val', 'map'])\n",
    "labels_pred_proc_mapped = labels_pred_proc.map(mapping.set_index('val')['map'])\n",
    "print(\"Validity Check (should be same number as before):\",adjusted_mutual_info_score(labels_pred_proc_mapped, labels_true_proc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Unpreprocessed Data**<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = Munkres()\n",
    "contmat = contingency_matrix(labels_true_unproc, labels_pred_unproc)\n",
    "mapping = pd.DataFrame(m.compute(contmat.max() - contmat), columns=['val', 'map'])\n",
    "labels_pred_unproc_mapped = labels_pred_unproc.map(mapping.set_index('val')['map'])\n",
    "print(\"Validity Check (should be same number as before):\",adjusted_mutual_info_score(labels_pred_unproc_mapped, labels_true_unproc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contingency Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure(num=None, figsize=(8, 6), dpi=80, facecolor='w', edgecolor='k')\n",
    "plt.clf()\n",
    "res = sns.heatmap(contingency_matrix(labels_true_proc, labels_pred_proc_mapped), fmt='.2f', cmap=\"YlGnBu\", vmin=0.0, vmax=100.0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(num=None, figsize=(8, 6), dpi=80, facecolor='w', edgecolor='k')\n",
    "plt.clf()\n",
    "res = sns.heatmap(contingency_matrix(labels_true_unproc, labels_pred_unproc_mapped), fmt='.2f', cmap=\"YlGnBu\", vmin=0.0, vmax=100.0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Silhouette Coefficient\n",
    "We will also calculate the silhouette coefficient to evaluate the quality of the clusters. The silhouette score ranges from -1 to 1, where a higher value indicates that the object is well matched to its own cluster and poorly matched to neighboring clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "\n",
    "\n",
    "X = time_embedded.loc[:,~time_embedded.columns.isin(['state', 'dataset'])]\n",
    "\n",
    "range_n_clusters = [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18]\n",
    "\n",
    "for n_clusters in range_n_clusters:\n",
    "    # Create a subplot with 1 row and 2 columns\n",
    "    fig, (ax1) = plt.subplots(1, 1)\n",
    "    fig.set_size_inches(18, 7)\n",
    "\n",
    "    # The 1st subplot is the silhouette plot\n",
    "    # The silhouette coefficient can range from -1, 1 but in this example all\n",
    "    # lie within [-0.1, 1]\n",
    "    ax1.set_xlim([-0.1, 1])\n",
    "    # The (n_clusters+1)*10 is for inserting blank space between silhouette\n",
    "    # plots of individual clusters, to demarcate them clearly.\n",
    "    ax1.set_ylim([0, len(X) + (n_clusters + 1) * 10])\n",
    "\n",
    "    # Initialize the clusterer with n_clusters value and a random generator\n",
    "    # seed of 10 for reproducibility.\n",
    "    clusterer = KMeans(n_clusters=n_clusters, random_state=10)\n",
    "    cluster_labels = clusterer.fit_predict(X)\n",
    "\n",
    "    # The silhouette_score gives the average value for all the samples.\n",
    "    # This gives a perspective into the density and separation of the formed\n",
    "    # clusters\n",
    "    silhouette_avg = silhouette_score(X, cluster_labels)\n",
    "    print(\n",
    "        \"For n_clusters =\",\n",
    "        n_clusters,\n",
    "        \"The average silhouette_score is :\",\n",
    "        silhouette_avg,\n",
    "    )\n",
    "\n",
    "    # Compute the silhouette scores for each sample\n",
    "    sample_silhouette_values = silhouette_samples(X, cluster_labels)\n",
    "\n",
    "    y_lower = 10\n",
    "    for i in range(n_clusters):\n",
    "        # Aggregate the silhouette scores for samples belonging to\n",
    "        # cluster i, and sort them\n",
    "        ith_cluster_silhouette_values = sample_silhouette_values[cluster_labels == i]\n",
    "\n",
    "        ith_cluster_silhouette_values.sort()\n",
    "\n",
    "        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
    "        y_upper = y_lower + size_cluster_i\n",
    "\n",
    "        color = cm.nipy_spectral(float(i) / n_clusters)\n",
    "        ax1.fill_betweenx(\n",
    "            np.arange(y_lower, y_upper),\n",
    "            0,\n",
    "            ith_cluster_silhouette_values,\n",
    "            facecolor=color,\n",
    "            edgecolor=color,\n",
    "            alpha=0.7,\n",
    "        )\n",
    "\n",
    "        # Label the silhouette plots with their cluster numbers at the middle\n",
    "        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
    "\n",
    "        # Compute the new y_lower for next plot\n",
    "        y_lower = y_upper + 10  # 10 for the 0 samples\n",
    "\n",
    "    ax1.set_title(\"The silhouette plot for the various clusters.\")\n",
    "    ax1.set_xlabel(\"The silhouette coefficient values\")\n",
    "    ax1.set_ylabel(\"Cluster label\")\n",
    "\n",
    "    # The vertical line for average silhouette score of all the values\n",
    "    ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
    "\n",
    "    ax1.set_yticks([])  # Clear the yaxis labels / ticks\n",
    "    ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n",
    "\n",
    "\n",
    "    plt.suptitle(\n",
    "        \"Silhouette analysis for KMeans clustering on sample data with n_clusters = %d\"\n",
    "        % n_clusters,\n",
    "        fontsize=14,\n",
    "        fontweight=\"bold\",\n",
    "    )\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Biplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode the dataset column as numerical values\n",
    "time_embedded_copy = time_embedded.copy()\n",
    "label_encoder = LabelEncoder()\n",
    "time_embedded_copy['dataset'] = label_encoder.fit_transform(time_embedded_copy['dataset'])\n",
    "time_embedded_copy.columns = time_embedded_copy.columns.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only take the ventral turns\n",
    "ventral_data = time_embedded_copy.loc[time_embedded_copy['state']=='ventral',:]\n",
    "pcav = PCA(n_components=3)\n",
    "ventral_pcs = pcav.fit_transform(ventral_data.loc[:,~ventral_data.columns.isin(['state', 'cluster'])]) # include the dataset column\n",
    "ventral_components = pcav.components_ # directions of maximum variance in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def myplot(score,coeff,labels=None):\n",
    "    xs = score[:,0]\n",
    "    ys = score[:,1]\n",
    "    n = coeff.shape[0]\n",
    "    scalex = 1.0/(xs.max() - xs.min())\n",
    "    scaley = 1.0/(ys.max() - ys.min())\n",
    "    plt.scatter(xs * scalex,ys * scaley, c = ventral_data['dataset'])\n",
    "    for i in range(n):\n",
    "        if i==740:\n",
    "            scale=1\n",
    "        else:\n",
    "            scale=5\n",
    "        plt.arrow(0, 0, coeff[i,0]*scale, coeff[i,1]*scale,color = 'r',alpha = 0.5)\n",
    "        if labels is None:\n",
    "            plt.text(coeff[i,0]* scale, coeff[i,1] * scale, \"Var\"+str(i+1), color = 'black', ha = 'center', va = 'center')\n",
    "        else:\n",
    "            plt.text(coeff[i,0]* 5, coeff[i,1] * 5, labels[i], color = 'b', ha = 'center', va = 'center')\n",
    "plt.xlim(-1,1)\n",
    "plt.ylim(-1,1)\n",
    "plt.xlabel(\"PC{}\".format(1))\n",
    "plt.ylabel(\"PC{}\".format(2))\n",
    "plt.grid()\n",
    "\n",
    "#Call the function. Use only the 2 PCs.\n",
    "myplot(ventral_pcs[:,0:3],np.transpose(ventral_components[0:3, :]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcav2 = PCA(n_components=3)\n",
    "ventral_pcs2 = pcav2.fit_transform(ventral_data.loc[:,~ventral_data.columns.isin(['state', 'dataset','cluster'])])\n",
    "ventral_components2 = pcav2.components_ # directions of maximum variance in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def myplot(score,coeff,labels=None):\n",
    "    xs = score[:,0]\n",
    "    ys = score[:,1]\n",
    "    n = coeff.shape[0]\n",
    "    scalex = 1.0/(xs.max() - xs.min())\n",
    "    scaley = 1.0/(ys.max() - ys.min())\n",
    "    plt.scatter(xs * scalex,ys * scaley, c = ventral_data['dataset'])\n",
    "    for i in range(n):\n",
    "        if i==740:\n",
    "            scale=1\n",
    "        else:\n",
    "            scale=5\n",
    "        plt.arrow(0, 0, coeff[i,0]*scale, coeff[i,1]*scale,color = 'r',alpha = 0.5)\n",
    "        if labels is None:\n",
    "            plt.text(coeff[i,0]* scale, coeff[i,1] * scale, \"Var\"+str(i+1), color = 'black', ha = 'center', va = 'center')\n",
    "        else:\n",
    "            plt.text(coeff[i,0]* 5, coeff[i,1] * 5, labels[i], color = 'b', ha = 'center', va = 'center')\n",
    "plt.xlim(-1,1)\n",
    "plt.ylim(-1,1)\n",
    "plt.xlabel(\"PC{}\".format(1))\n",
    "plt.ylabel(\"PC{}\".format(2))\n",
    "plt.grid()\n",
    "\n",
    "#Call the function. Use only the 2 PCs.\n",
    "myplot(ventral_pcs2[:,0:3],np.transpose(pcav2.components_[0:3, :]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantification of variability within state with Median Points\n",
    "\n",
    "We want to perform PCA on the ventral state points and identify the direction (eigenvector) that captures a lot of variance but across different trajectories and not just within a single trajectory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ventral_te = time_embedded_copy.copy()\n",
    "\n",
    "pca_te = PCA(n_components=3)\n",
    "ventral_te_pc = pca_te.fit_transform(ventral_te.loc[:,~ventral_te.columns.isin(['state','dataset', 'cluster'])])\n",
    "ventral_te_pc_df = pd.DataFrame(ventral_te_pc)\n",
    "\n",
    "turn_vec = time_embedded_copy['state'].values\n",
    "ventral_te_pc_df['dataset'] = data['dataset'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_names = []\n",
    "intervals = []\n",
    "is_ventral = False\n",
    "count = 0\n",
    "for i in range(ventral_te_pc.shape[0]):\n",
    "    if turn_vec[i] == 'ventral':\n",
    "        if is_ventral:\n",
    "            continue\n",
    "        else:\n",
    "            start_idx=i\n",
    "            is_ventral = True\n",
    "        continue\n",
    "    else:\n",
    "        if not is_ventral:\n",
    "            continue\n",
    "        else:\n",
    "            end_idx=i-1\n",
    "            array = ventral_te_pc_df.loc[start_idx:end_idx,~ventral_te_pc_df.columns.isin(['dataset'])]\n",
    "            dataset_names.append(ventral_te_pc_df['dataset'].loc[start_idx])\n",
    "            intervals.append(array)\n",
    "            is_ventral = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_medians = []\n",
    "for i in intervals:\n",
    "    quan = i.loc[int(np.median(i.index, axis=0)), :]\n",
    "    all_medians.append(quan)\n",
    "    \n",
    "pca_median = PCA(n_components=3)\n",
    "median_pcs = pca_median.fit_transform(pd.DataFrame(all_medians))\n",
    "median_pc1 = median_pcs[:,0]\n",
    "median_comps = pca_median.components_\n",
    "#med = list(np.concatenate(median_pcs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "med_df = pd.DataFrame(median_pc1, columns=['Median'])\n",
    "med_df[\"dataset\"] = dataset_names\n",
    "med_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(data=med_df, x=\"Median\",y=\"dataset\",hue='dataset',bins=250, legend=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(median_pc1, bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vtp = pd.DataFrame(ventral_te_pc)\n",
    "vtp['state'] = turn_vec\n",
    "\n",
    "fig = hf.plot_PCs(vtp)\n",
    "\n",
    "for i in range(len(all_medians)):\n",
    "    fig.add_trace(go.Scatter3d(x=[all_medians[i][0]],\n",
    "                                y=[all_medians[i][1]],\n",
    "                                z=[all_medians[i][2]],\n",
    "                                mode='markers',\n",
    "                                marker=dict(color='black', size=3)))\n",
    "    \n",
    "# scale the components\n",
    "scaled_ventral_components = np.zeros(median_comps.shape)\n",
    "max_coord = np.abs(ventral_te_pc).max(axis=1).max()\n",
    "scaled_ventral_components[0]=median_comps[0]*max_coord\n",
    "\n",
    "fig.add_trace(go.Scatter3d(x=[-scaled_ventral_components[0, 0], scaled_ventral_components[0, 0]],\n",
    "                            y=[-scaled_ventral_components[0, 1], scaled_ventral_components[0, 1]],\n",
    "                            z=[-scaled_ventral_components[0, 2], scaled_ventral_components[0, 2]],\n",
    "                            mode='lines', name=f'Principal Component 1',\n",
    "                            line=dict(color='black', width=3)))\n",
    "    \n",
    "fig.update_xaxes(type='linear')\n",
    "fig.update_yaxes(type='linear')\n",
    "fig.update_layout(title='PCA of time-embedded data')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification\n",
    "\n",
    "We will now try to classify the preprocessed data based on the dataset membership.\n",
    "\n",
    "Note: Since cross_val_predict does not work with TimeSplit we will use custom code from stackexchange (Marco Cerliani)\n",
    "\n",
    "### State Classification on Original (Unpreprocessed) Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tscv = TimeSeriesSplit()\n",
    "\n",
    "X = imputed_dataframe.loc[:,~imputed_dataframe.columns.isin(['state', 'dataset', 'cluster'])]\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y =  label_encoder.fit_transform(imputed_dataframe['state'])\n",
    "\n",
    "prec = make_scorer(metrics.precision_score, average='weighted')\n",
    "reca = make_scorer(metrics.recall_score, average='weighted')\n",
    "f1 = make_scorer(metrics.f1_score, average='weighted')\n",
    "acc = make_scorer(metrics.accuracy_score)\n",
    "scoring={\"accuracy\":acc, \"precision\":prec, \"recall\":reca, \"f1\":f1}\n",
    "\n",
    "cv_results_original = cross_validate(SVC(gamma='auto'), X, y, cv=tscv, scoring=scoring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results_original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for l in list(cv_results_original.values()):\n",
    "    print(np.mean(l))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### State Classification on Preprocessed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tscv = TimeSeriesSplit()\n",
    "\n",
    "X = time_embedded.loc[:,~time_embedded.columns.isin(['state', 'dataset', 'cluster'])]\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y =  label_encoder.fit_transform(time_embedded['state'])\n",
    "\n",
    "cv_results_preprocessed = cross_validate(SVC(gamma='auto'), X, y, cv=tscv, scoring=scoring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results_preprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for l in list(cv_results_preprocessed.values()):\n",
    "    print(np.mean(l))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### State Classification on PCA Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tscv = TimeSeriesSplit()\n",
    "\n",
    "X = data.loc[:,~data.columns.isin(['state', 'dataset', 'cluster'])]\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y =  label_encoder.fit_transform(data['state'])\n",
    "\n",
    "\n",
    "cv_results_pca = cross_validate(SVC(gamma='auto'), X, y, cv=tscv, scoring=scoring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for l in list(cv_results_pca.values()):\n",
    "    print(np.mean(l))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Membership Classification on Original Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection._split import _BaseKFold, indexable, _num_samples\n",
    "from sklearn.utils.validation import _deprecate_positional_args\n",
    "\n",
    "# https://github.com/getgaurav2/scikit-learn/blob/d4a3af5cc9da3a76f0266932644b884c99724c57/sklearn/model_selection/_split.py#L2243\n",
    "class GroupTimeSeriesSplit(_BaseKFold):\n",
    "    \"\"\"Time Series cross-validator variant with non-overlapping groups.\n",
    "    Provides train/test indices to split time series data samples\n",
    "    that are observed at fixed time intervals according to a\n",
    "    third-party provided group.\n",
    "    In each split, test indices must be higher than before, and thus shuffling\n",
    "    in cross validator is inappropriate.\n",
    "    This cross-validation object is a variation of :class:`KFold`.\n",
    "    In the kth split, it returns first k folds as train set and the\n",
    "    (k+1)th fold as test set.\n",
    "    The same group will not appear in two different folds (the number of\n",
    "    distinct groups has to be at least equal to the number of folds).\n",
    "    Note that unlike standard cross-validation methods, successive\n",
    "    training sets are supersets of those that come before them.\n",
    "    Read more in the :ref:`User Guide <cross_validation>`.\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_splits : int, default=5\n",
    "        Number of splits. Must be at least 2.\n",
    "    max_train_size : int, default=None\n",
    "        Maximum size for a single training set.\n",
    "    Examples\n",
    "    --------\n",
    "    >>> import numpy as np\n",
    "    >>> from sklearn.model_selection import GroupTimeSeriesSplit\n",
    "    >>> groups = np.array(['a', 'a', 'a', 'a', 'a', 'a',\\\n",
    "                           'b', 'b', 'b', 'b', 'b',\\\n",
    "                           'c', 'c', 'c', 'c',\\\n",
    "                           'd', 'd', 'd'])\n",
    "    >>> gtss = GroupTimeSeriesSplit(n_splits=3)\n",
    "    >>> for train_idx, test_idx in gtss.split(groups, groups=groups):\n",
    "    ...     print(\"TRAIN:\", train_idx, \"TEST:\", test_idx)\n",
    "    ...     print(\"TRAIN GROUP:\", groups[train_idx],\\\n",
    "                  \"TEST GROUP:\", groups[test_idx])\n",
    "    TRAIN: [0, 1, 2, 3, 4, 5] TEST: [6, 7, 8, 9, 10]\n",
    "    TRAIN GROUP: ['a' 'a' 'a' 'a' 'a' 'a']\\\n",
    "    TEST GROUP: ['b' 'b' 'b' 'b' 'b']\n",
    "    TRAIN: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10] TEST: [11, 12, 13, 14]\n",
    "    TRAIN GROUP: ['a' 'a' 'a' 'a' 'a' 'a' 'b' 'b' 'b' 'b' 'b']\\\n",
    "    TEST GROUP: ['c' 'c' 'c' 'c']\n",
    "    TRAIN: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\\\n",
    "    TEST: [15, 16, 17]\n",
    "    TRAIN GROUP: ['a' 'a' 'a' 'a' 'a' 'a' 'b' 'b' 'b' 'b' 'b' 'c' 'c' 'c' 'c']\\\n",
    "    TEST GROUP: ['d' 'd' 'd']\n",
    "    \"\"\"\n",
    "    @_deprecate_positional_args\n",
    "    def __init__(self,\n",
    "                 n_splits=5,\n",
    "                 *,\n",
    "                 max_train_size=None\n",
    "                 ):\n",
    "        super().__init__(n_splits, shuffle=False, random_state=None)\n",
    "        self.max_train_size = max_train_size\n",
    "\n",
    "    def split(self, X, y=None, groups=None):\n",
    "        \"\"\"Generate indices to split data into training and test set.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "            Training data, where n_samples is the number of samples\n",
    "            and n_features is the number of features.\n",
    "        y : array-like of shape (n_samples,)\n",
    "            Always ignored, exists for compatibility.\n",
    "        groups : array-like of shape (n_samples,)\n",
    "            Group labels for the samples used while splitting the dataset into\n",
    "            train/test set.\n",
    "        Yields\n",
    "        ------\n",
    "        train : ndarray\n",
    "            The training set indices for that split.\n",
    "        test : ndarray\n",
    "            The testing set indices for that split.\n",
    "        \"\"\"\n",
    "        if groups is None:\n",
    "            raise ValueError(\n",
    "                \"The 'groups' parameter should not be None\")\n",
    "        X, y, groups = indexable(X, y, groups)\n",
    "        n_samples = _num_samples(X)\n",
    "        n_splits = self.n_splits\n",
    "        n_folds = n_splits + 1\n",
    "        group_dict = {}\n",
    "        u, ind = np.unique(groups, return_index=True)\n",
    "        unique_groups = u[np.argsort(ind)]\n",
    "        n_samples = _num_samples(X)\n",
    "        n_groups = _num_samples(unique_groups)\n",
    "        for idx in np.arange(n_samples):\n",
    "            if (groups[idx] in group_dict):\n",
    "                group_dict[groups[idx]].append(idx)\n",
    "            else:\n",
    "                group_dict[groups[idx]] = [idx]\n",
    "        if n_folds > n_groups:\n",
    "            raise ValueError(\n",
    "                (\"Cannot have number of folds={0} greater than\"\n",
    "                 \" the number of groups={1}\").format(n_folds,\n",
    "                                                     n_groups))\n",
    "        group_test_size = n_groups // n_folds\n",
    "        group_test_starts = range(n_groups - n_splits * group_test_size,\n",
    "                                  n_groups, group_test_size)\n",
    "        for group_test_start in group_test_starts:\n",
    "            train_array = []\n",
    "            test_array = []\n",
    "            for train_group_idx in unique_groups[:group_test_start]:\n",
    "                train_array_tmp = group_dict[train_group_idx]\n",
    "                train_array = np.sort(np.unique(\n",
    "                                      np.concatenate((train_array,\n",
    "                                                      train_array_tmp)),\n",
    "                                      axis=None), axis=None)\n",
    "            train_end = train_array.size\n",
    "            if self.max_train_size and self.max_train_size < train_end:\n",
    "                train_array = train_array[train_end -\n",
    "                                          self.max_train_size:train_end]\n",
    "            for test_group_idx in unique_groups[group_test_start:\n",
    "                                                group_test_start +\n",
    "                                                group_test_size]:\n",
    "                test_array_tmp = group_dict[test_group_idx]\n",
    "                test_array = np.sort(np.unique(\n",
    "                                              np.concatenate((test_array,\n",
    "                                                              test_array_tmp)),\n",
    "                                     axis=None), axis=None)\n",
    "            yield [int(i) for i in train_array], [int(i) for i in test_array]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tscv = GroupTimeSeriesSplit(n_splits=5)\n",
    "X = data.loc[:,~data.columns.isin(['state', 'dataset', 'cluster'])]\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y =  label_encoder.fit_transform(data['dataset'])\n",
    "\n",
    "\n",
    "cv_results_pca = cross_validate(SVC(gamma='auto'), X, y, cv=tscv, groups=data['dataset'],scoring=scoring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tscv = TimeSeriesSplit() # expanding window cross-validation\n",
    "\n",
    "train_sets = []\n",
    "test_sets = []\n",
    "\n",
    "groups = time_embedded.groupby('dataset')\n",
    "for name, group in groups:\n",
    "    group = group.reset_index()\n",
    "    for i, (train_index, test_index) in enumerate(tscv.split(group.loc[:,~group.columns.isin(['state', 'dataset'])])):\n",
    "        train_sets.append(group.loc[train_index,~group.columns.isin(['state'])])\n",
    "        test_sets.append(group.loc[test_index,~group.columns.isin(['state'])])\n",
    "train_X = pd.concat(train_sets)\n",
    "test_X = pd.concat(test_sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = train_X['dataset']\n",
    "train_X = train_X.drop(columns=['index'])\n",
    "test_X = test_X.drop(columns=['index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = SVC(gamma='auto')\n",
    "clf.fit(train_X.loc[:,~train_X.columns.isin(['state', 'dataset'])], train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = test_X['dataset']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(pd.DataFrame(test_X.loc[:,~test_X.columns.isin(['state', 'dataset'])]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "# Model Accuracy: how often is the classifier correct?\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "# Model Precision: what percentage of positive tuples are labeled as such?\n",
    "print(\"Precision:\",metrics.precision_score(y_test, y_pred, average='weighted'))\n",
    "\n",
    "# Model Recall: what percentage of positive tuples are labelled as such?\n",
    "print(\"Recall:\",metrics.recall_score(y_test, y_pred, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred, labels=clf.classes_)\n",
    "ax= plt.subplot()\n",
    "sns.heatmap(cm, annot=False, fmt='g', ax=ax);  #annot=True to annotate cells, ftm='g' to disable scientific notation\n",
    "\n",
    "# labels, title and ticks\n",
    "ax.set_xlabel('Predicted labels');ax.set_ylabel('True labels'); \n",
    "ax.set_title('Confusion Matrix'); \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Membership with unpreprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tscv = TimeSeriesSplit() # expanding window cross-validation\n",
    "\n",
    "train_sets = []\n",
    "test_sets = []\n",
    "\n",
    "groups = imputed_dataframe.groupby('dataset')\n",
    "for name, group in groups:\n",
    "    group = group.reset_index()\n",
    "    for i, (train_index, test_index) in enumerate(tscv.split(group.loc[:,~group.columns.isin(['state', 'dataset','cluster'])])):\n",
    "        train_sets.append(group.loc[train_index,~group.columns.isin(['state','cluster'])])\n",
    "        test_sets.append(group.loc[test_index,~group.columns.isin(['state','cluster'])])\n",
    "train_X = pd.concat(train_sets)\n",
    "test_X = pd.concat(test_sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = train_X['dataset']\n",
    "test_y = test_X['dataset']\n",
    "train_X = train_X.drop(columns=['index'])\n",
    "test_X = test_X.drop(columns=['index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = SVC(gamma='auto')\n",
    "clf.fit(train_X.loc[:,~train_X.columns.isin(['state', 'dataset'])], train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(pd.DataFrame(test_X.loc[:,~test_X.columns.isin(['dataset'])]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Accuracy: how often is the classifier correct?\n",
    "print(\"Accuracy:\",metrics.accuracy_score(test_y, y_pred))\n",
    "\n",
    "# Model Precision: what percentage of positive tuples are labeled as such?\n",
    "print(\"Precision:\",metrics.precision_score(test_y, y_pred, average='weighted'))\n",
    "\n",
    "# Model Recall: what percentage of positive tuples are labelled as such?\n",
    "print(\"Recall:\",metrics.recall_score(test_y, y_pred, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(test_y, y_pred, labels=clf.classes_)\n",
    "ax= plt.subplot()\n",
    "sns.heatmap(cm, annot=False, fmt='g', ax=ax);  #annot=True to annotate cells, ftm='g' to disable scientific notation\n",
    "\n",
    "# labels, title and ticks\n",
    "ax.set_xlabel('Predicted labels');ax.set_ylabel('True labels'); \n",
    "ax.set_title('Confusion Matrix'); "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Behaviour States"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tscv = TimeSeriesSplit() # expanding window cross-validation\n",
    "\n",
    "train_sets_st = []\n",
    "test_sets_st = []\n",
    "\n",
    "groups = time_embedded.groupby('dataset')\n",
    "for name, group in groups:\n",
    "    group = group.reset_index()\n",
    "    for i, (train_index, test_index) in enumerate(tscv.split(group.loc[:,~group.columns.isin(['state', 'dataset'])])):\n",
    "        train_sets_st.append(group.loc[train_index,~group.columns.isin(['dataset'])])\n",
    "        test_sets_st.append(group.loc[test_index,~group.columns.isin(['dataset'])])\n",
    "train_X = pd.concat(train_sets_st)\n",
    "test_X = pd.concat(test_sets_st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = train_X['state']\n",
    "train_X = train_X.drop(columns=['index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = SVC(gamma='auto')\n",
    "clf.fit(train_X.loc[:,~train_X.columns.isin(['state', 'dataset'])], train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X = test_X.drop(columns=['index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(pd.DataFrame(test_X.loc[:,~test_X.columns.isin(['state', 'dataset'])]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = test_X['state']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Accuracy: how often is the classifier correct?\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "# Model Precision: what percentage of positive tuples are labeled as such?\n",
    "print(\"Precision:\",metrics.precision_score(y_test, y_pred, average='weighted'))\n",
    "\n",
    "# Model Recall: what percentage of positive tuples are labelled as such?\n",
    "print(\"Recall:\",metrics.recall_score(y_test, y_pred, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred, labels=clf.classes_)\n",
    "ax= plt.subplot()\n",
    "sns.heatmap(cm, annot=False, fmt='g', ax=ax);  #annot=True to annotate cells, ftm='g' to disable scientific notation\n",
    "\n",
    "# labels, title and ticks\n",
    "ax.set_xlabel('Predicted labels');ax.set_ylabel('True labels'); \n",
    "ax.xaxis.set_ticklabels(clf.classes_); ax.yaxis.set_ticklabels(clf.classes_.T)\n",
    "plt.xticks(rotation=45)\n",
    "plt.yticks(rotation=45)\n",
    "ax.set_title('Confusion Matrix'); \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recurrence Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "te = np.asarray(data.loc[:,~data.columns.isin(['state', 'dataset'])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_Y = np.linalg.norm(te[:20000, np.newaxis] - te[:20000,:], axis=-1)\n",
    "plt.matshow(pd_Y, cmap='Greys')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "def recurrence_plot(data, threshold=0.1):\n",
    "    \"\"\"\n",
    "    Generate a recurrence plot from a time series.\n",
    "\n",
    "    :param data: Time series data\n",
    "    :param threshold: Threshold to determine recurrence\n",
    "    :return: Recurrence plot\n",
    "    \"\"\"\n",
    "    # Calculate the distance matrix\n",
    "    N = len(data)\n",
    "    distance_matrix = np.zeros((N, N))\n",
    "    count = 0\n",
    "    for i in tqdm(range(N)):\n",
    "        for j in range(N):\n",
    "            distance_matrix[i, j] = np.linalg.norm(data[i] - data[j]) # euclidean distance between two points\n",
    "            if distance_matrix[i, j] <= threshold:\n",
    "                count += 1\n",
    "\n",
    "    # Create the recurrence plot\n",
    "    recurrence_plot = np.where(distance_matrix <= threshold, 1, 0)\n",
    "    print(count)\n",
    "    return recurrence_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate and plot the recurrence plot of the first principal component\n",
    "recurrence = recurrence_plot(np.array(data.loc[:6000,0]), threshold=0.8) # run time and memory allocation for full dataset is too high \n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(recurrence, cmap='Greys', origin='lower')\n",
    "plt.title('Recurrence Plot')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Time')\n",
    "plt.colorbar(label='Recurrence')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import pdist, squareform\n",
    "dist = pdist(te[:10000, :])\n",
    "dist = squareform(dist)\n",
    "sns.heatmap(dist, cmap=\"mako\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = data.groupby('dataset')\n",
    "all_dfs = []\n",
    "for name, group in groups:\n",
    "    df = group.reset_index().loc[:900,:]\n",
    "    all_dfs.append(df)\n",
    "data_truncated = pd.concat(all_dfs)\n",
    "te_trunc = np.asarray(data_truncated.loc[:,~data_truncated.columns.isin(['state', 'dataset'])])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import pdist, squareform\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "dist = pdist(te_trunc)\n",
    "dist = squareform(dist)\n",
    "sns.heatmap(dist, cmap=\"mako\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix I: More Clustering\n",
    "\n",
    "### 5 clusters and comparing with state membership"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=5)\n",
    "state_clustering = data.copy()\n",
    "state_clustering['cluster'] = kmeans.fit_predict(time_embedded.loc[:,~time_embedded.columns.isin(['state', 'dataset'])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Assuming 'text_column' is your pandas DataFrame column with text values\n",
    "label_encoder = LabelEncoder()\n",
    "state_clustering['dataset_numeric'] = label_encoder.fit_transform(state_clustering['dataset'])\n",
    "\n",
    "labels_pred_proc = state_clustering['cluster']\n",
    "labels_true_proc = state_clustering['dataset_numeric']\n",
    "\n",
    "# Calculate adjusted mutual information score, which tells us how well the clustering results match the ground truth\n",
    "print(adjusted_mutual_info_score(labels_true_proc, labels_pred_proc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is little correspondence between the clusters and the state membership. This suggests that the clustering is not based on the state membership.\n",
    "\n",
    "### 23 clusters but within each state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = time_embedded['state'].unique().tolist()\n",
    "for state in states:\n",
    "    kmeans = KMeans(n_clusters=23)\n",
    "    labels_pred_proc = kmeans.fit_predict(time_embedded.loc[time_embedded['state']==state,~time_embedded.columns.isin(['state', 'dataset'])])\n",
    "    labels_true_proc = state_clustering.loc[state_clustering['state']==state,'dataset_numeric']\n",
    "    print(state,\"\\n adj. mutual info\",adjusted_mutual_info_score(labels_true_proc, labels_pred_proc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# control\n",
    "for state in states:\n",
    "    kmeans = KMeans(n_clusters=23)\n",
    "    labels_pred_proc = kmeans.fit_predict(imputed_dataframe.loc[imputed_dataframe['state']==state,~imputed_dataframe.columns.isin(['state', 'dataset'])])\n",
    "    labels_true_proc = imputed_dataframe_copy.loc[imputed_dataframe_copy['state']==state,'dataset_numeric']\n",
    "    print(state,\"\\n adj. mutual info\",adjusted_mutual_info_score(labels_true_proc, labels_pred_proc))\n",
    "    fig = plt.figure(num=None, figsize=(8, 6), dpi=80, facecolor='w', edgecolor='k')\n",
    "    plt.clf()\n",
    "    res = sns.heatmap(contingency_matrix(labels_true_proc, labels_pred_proc), fmt='.2f', cmap=\"YlGnBu\", vmin=0.0, vmax=100.0)\n",
    "    plt.show()\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "undefined.undefined.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
