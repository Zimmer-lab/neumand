{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Diagnostics: Variability of the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "import helper_functions as hf\n",
    "import plotly.graph_objects as go\n",
    "from sklearn.metrics.cluster import contingency_matrix\n",
    "from munkres import Munkres\n",
    "import seaborn as sns\n",
    "sns.set_theme()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_embedded = pd.read_hdf('time_embedded_2103.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=3)\n",
    "pca_data = pca.fit_transform(time_embedded.loc[:,~time_embedded.columns.isin(['state', 'dataset'])])\n",
    "data = pd.DataFrame(pca_data)\n",
    "data['state'] = time_embedded['state']\n",
    "data['dataset'] = time_embedded['dataset']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = {dataset: df for dataset, df in data.groupby('dataset')}\n",
    "hf.plot_PCs_separately(datasets).run_server(debug=True, port=8054)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Color-code trajectories based on dataset \n",
    "Each data point is colored based on the dataset it belongs to. This helps in understanding the variability of the data across different datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_traces = []\n",
    "\n",
    "for name, df in datasets.items():\n",
    "    trace = go.Scatter3d(x=df[0], y=df[1], z=df[2], mode=\"lines\", name=name)\n",
    "    all_traces.append(trace)\n",
    "    \n",
    "fig = go.Figure(data=all_traces)\n",
    "\n",
    "variances = pca.explained_variance_ratio_ * 100\n",
    "scene = dict(xaxis_title=f\"PC 1 ({variances[0]:.2f}%)\",\n",
    "                yaxis_title=f\"PC 2 ({variances[1]:.2f}%)\",\n",
    "                zaxis_title=f\"PC 3 ({variances[2]:.2f}%)\")\n",
    "\n",
    "fig.update_layout(scene=scene)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering with KMeans \n",
    "We can try clustering our data without the dataset feature to see if the separation of data points is based on the dataset membership or not.\n",
    "\n",
    "We could cross check with a dataset where no preprocessing has been done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "kmeans = KMeans(n_clusters=23)\n",
    "data['cluster'] = kmeans.fit_predict(time_embedded.loc[:,~time_embedded.columns.isin(['state', 'dataset'])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "data_copy = data.copy()\n",
    "# Assuming 'text_column' is your pandas DataFrame column with text values\n",
    "label_encoder = LabelEncoder()\n",
    "data_copy['dataset_numeric'] = label_encoder.fit_transform(data_copy['dataset'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_copy.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluation** <br>\n",
    "We can evaluate the clustering using the adjusted mutual information score, which calculates the mutual information between two clusterings and then normalizes this value by the expected mutual information of two random clusterings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.cluster import adjusted_mutual_info_score\n",
    "\n",
    "labels_pred_proc = data_copy['cluster']\n",
    "labels_true_proc = data_copy['dataset_numeric']\n",
    "\n",
    "# Calculate adjusted mutual information score, which tells us how well the clustering results match the ground truth\n",
    "print(adjusted_mutual_info_score(labels_true_proc, labels_pred_proc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Control Dataset\n",
    "We will now cluster our unpreprocessed data and see if the clusters are based on the dataset membership or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputed_dataframe = pd.read_hdf(\"imputed_dataframe_0602.h5\")\n",
    "kmeans2 = KMeans(n_clusters=23)\n",
    "imputed_dataframe['cluster'] = kmeans2.fit_predict(imputed_dataframe.loc[:,~imputed_dataframe.columns.isin(['state', 'dataset'])])\n",
    "labels_pred_unproc = imputed_dataframe['cluster']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputed_dataframe_copy = imputed_dataframe.copy()\n",
    "# Assuming 'text_column' is your pandas DataFrame column with text values\n",
    "label_encoder2 = LabelEncoder()\n",
    "imputed_dataframe_copy['dataset_numeric'] = label_encoder2.fit_transform(imputed_dataframe_copy['dataset'])\n",
    "labels_true_unproc = imputed_dataframe_copy['dataset_numeric']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(adjusted_mutual_info_score(imputed_dataframe_copy['cluster'], imputed_dataframe_copy['dataset_numeric']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Result**<br>\n",
    "Since the adjusted mutual information score between the clustering of our data and the dataset membership is relatively low (also compared to the unpreprocessed data), we can assume that the separation of data points is not based on the dataset membership and that the dataset feature might not explain the variability of the trajectories.\n",
    "\n",
    "\n",
    "#### Cluster Label Correspondence\n",
    "We want to match the predicted cluster labels to the dataset labels to see if the clustering is consistent with the dataset membership.\n",
    "\n",
    "**Preprocessed Data**<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = Munkres()\n",
    "contmat = contingency_matrix(labels_true_proc, labels_pred_proc)\n",
    "mapping = pd.DataFrame(m.compute(contmat.max() - contmat), columns=['val', 'map'])\n",
    "labels_pred_proc_mapped = labels_pred_proc.map(mapping.set_index('val')['map'])\n",
    "print(\"Validity Check (should be same number as before):\",adjusted_mutual_info_score(labels_pred_proc_mapped, labels_true_proc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Unpreprocessed Data**<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = Munkres()\n",
    "contmat = contingency_matrix(labels_true_unproc, labels_pred_unproc)\n",
    "mapping = pd.DataFrame(m.compute(contmat.max() - contmat), columns=['val', 'map'])\n",
    "labels_pred_unproc_mapped = labels_pred_unproc.map(mapping.set_index('val')['map'])\n",
    "print(\"Validity Check (should be same number as before):\",adjusted_mutual_info_score(labels_pred_unproc_mapped, labels_true_unproc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contingency Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure(num=None, figsize=(8, 6), dpi=80, facecolor='w', edgecolor='k')\n",
    "plt.clf()\n",
    "res = sns.heatmap(contingency_matrix(labels_true_proc, labels_pred_proc_mapped), fmt='.2f', cmap=\"YlGnBu\", vmin=0.0, vmax=100.0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(num=None, figsize=(8, 6), dpi=80, facecolor='w', edgecolor='k')\n",
    "plt.clf()\n",
    "res = sns.heatmap(contingency_matrix(labels_true_unproc, labels_pred_unproc_mapped), fmt='.2f', cmap=\"YlGnBu\", vmin=0.0, vmax=100.0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Silhouette Coefficient\n",
    "We will also calculate the silhouette coefficient to evaluate the quality of the clusters. The silhouette score ranges from -1 to 1, where a higher value indicates that the object is well matched to its own cluster and poorly matched to neighboring clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "\n",
    "\n",
    "X = time_embedded.loc[:,~time_embedded.columns.isin(['state', 'dataset'])]\n",
    "\n",
    "range_n_clusters = [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18]\n",
    "\n",
    "for n_clusters in range_n_clusters:\n",
    "    # Create a subplot with 1 row and 2 columns\n",
    "    fig, (ax1) = plt.subplots(1, 1)\n",
    "    fig.set_size_inches(18, 7)\n",
    "\n",
    "    # The 1st subplot is the silhouette plot\n",
    "    # The silhouette coefficient can range from -1, 1 but in this example all\n",
    "    # lie within [-0.1, 1]\n",
    "    ax1.set_xlim([-0.1, 1])\n",
    "    # The (n_clusters+1)*10 is for inserting blank space between silhouette\n",
    "    # plots of individual clusters, to demarcate them clearly.\n",
    "    ax1.set_ylim([0, len(X) + (n_clusters + 1) * 10])\n",
    "\n",
    "    # Initialize the clusterer with n_clusters value and a random generator\n",
    "    # seed of 10 for reproducibility.\n",
    "    clusterer = KMeans(n_clusters=n_clusters, random_state=10)\n",
    "    cluster_labels = clusterer.fit_predict(X)\n",
    "\n",
    "    # The silhouette_score gives the average value for all the samples.\n",
    "    # This gives a perspective into the density and separation of the formed\n",
    "    # clusters\n",
    "    silhouette_avg = silhouette_score(X, cluster_labels)\n",
    "    print(\n",
    "        \"For n_clusters =\",\n",
    "        n_clusters,\n",
    "        \"The average silhouette_score is :\",\n",
    "        silhouette_avg,\n",
    "    )\n",
    "\n",
    "    # Compute the silhouette scores for each sample\n",
    "    sample_silhouette_values = silhouette_samples(X, cluster_labels)\n",
    "\n",
    "    y_lower = 10\n",
    "    for i in range(n_clusters):\n",
    "        # Aggregate the silhouette scores for samples belonging to\n",
    "        # cluster i, and sort them\n",
    "        ith_cluster_silhouette_values = sample_silhouette_values[cluster_labels == i]\n",
    "\n",
    "        ith_cluster_silhouette_values.sort()\n",
    "\n",
    "        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
    "        y_upper = y_lower + size_cluster_i\n",
    "\n",
    "        color = cm.nipy_spectral(float(i) / n_clusters)\n",
    "        ax1.fill_betweenx(\n",
    "            np.arange(y_lower, y_upper),\n",
    "            0,\n",
    "            ith_cluster_silhouette_values,\n",
    "            facecolor=color,\n",
    "            edgecolor=color,\n",
    "            alpha=0.7,\n",
    "        )\n",
    "\n",
    "        # Label the silhouette plots with their cluster numbers at the middle\n",
    "        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
    "\n",
    "        # Compute the new y_lower for next plot\n",
    "        y_lower = y_upper + 10  # 10 for the 0 samples\n",
    "\n",
    "    ax1.set_title(\"The silhouette plot for the various clusters.\")\n",
    "    ax1.set_xlabel(\"The silhouette coefficient values\")\n",
    "    ax1.set_ylabel(\"Cluster label\")\n",
    "\n",
    "    # The vertical line for average silhouette score of all the values\n",
    "    ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
    "\n",
    "    ax1.set_yticks([])  # Clear the yaxis labels / ticks\n",
    "    ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n",
    "\n",
    "\n",
    "    plt.suptitle(\n",
    "        \"Silhouette analysis for KMeans clustering on sample data with n_clusters = %d\"\n",
    "        % n_clusters,\n",
    "        fontsize=14,\n",
    "        fontweight=\"bold\",\n",
    "    )\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Biplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode the dataset column as numerical values\n",
    "time_embedded_copy = time_embedded.copy()\n",
    "label_encoder = LabelEncoder()\n",
    "time_embedded_copy['dataset'] = label_encoder.fit_transform(time_embedded_copy['dataset'])\n",
    "time_embedded_copy.columns = time_embedded_copy.columns.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only take the ventral turns\n",
    "ventral_data = time_embedded_copy.loc[time_embedded_copy['state']=='ventral',:]\n",
    "pcav = PCA(n_components=3)\n",
    "ventral_pcs = pcav.fit_transform(ventral_data.loc[:,~ventral_data.columns.isin(['state', 'cluster'])]) # include the dataset column\n",
    "ventral_components = pcav.components_ # directions of maximum variance in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def myplot(score,coeff,labels=None):\n",
    "    xs = score[:,0]\n",
    "    ys = score[:,1]\n",
    "    n = coeff.shape[0]\n",
    "    scalex = 1.0/(xs.max() - xs.min())\n",
    "    scaley = 1.0/(ys.max() - ys.min())\n",
    "    plt.scatter(xs * scalex,ys * scaley, c = ventral_data['dataset'])\n",
    "    for i in range(n):\n",
    "        if i==740:\n",
    "            scale=1\n",
    "        else:\n",
    "            scale=5\n",
    "        plt.arrow(0, 0, coeff[i,0]*scale, coeff[i,1]*scale,color = 'r',alpha = 0.5)\n",
    "        if labels is None:\n",
    "            plt.text(coeff[i,0]* scale, coeff[i,1] * scale, \"Var\"+str(i+1), color = 'black', ha = 'center', va = 'center')\n",
    "        else:\n",
    "            plt.text(coeff[i,0]* 5, coeff[i,1] * 5, labels[i], color = 'b', ha = 'center', va = 'center')\n",
    "plt.xlim(-1,1)\n",
    "plt.ylim(-1,1)\n",
    "plt.xlabel(\"PC{}\".format(1))\n",
    "plt.ylabel(\"PC{}\".format(2))\n",
    "plt.grid()\n",
    "\n",
    "#Call the function. Use only the 2 PCs.\n",
    "myplot(ventral_pcs[:,0:3],np.transpose(ventral_components[0:3, :]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcav2 = PCA(n_components=3)\n",
    "ventral_pcs2 = pcav2.fit_transform(ventral_data.loc[:,~ventral_data.columns.isin(['state', 'dataset','cluster'])])\n",
    "ventral_components2 = pcav2.components_ # directions of maximum variance in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def myplot(score,coeff,labels=None):\n",
    "    xs = score[:,0]\n",
    "    ys = score[:,1]\n",
    "    n = coeff.shape[0]\n",
    "    scalex = 1.0/(xs.max() - xs.min())\n",
    "    scaley = 1.0/(ys.max() - ys.min())\n",
    "    plt.scatter(xs * scalex,ys * scaley, c = ventral_data['dataset'])\n",
    "    for i in range(n):\n",
    "        if i==740:\n",
    "            scale=1\n",
    "        else:\n",
    "            scale=5\n",
    "        plt.arrow(0, 0, coeff[i,0]*scale, coeff[i,1]*scale,color = 'r',alpha = 0.5)\n",
    "        if labels is None:\n",
    "            plt.text(coeff[i,0]* scale, coeff[i,1] * scale, \"Var\"+str(i+1), color = 'black', ha = 'center', va = 'center')\n",
    "        else:\n",
    "            plt.text(coeff[i,0]* 5, coeff[i,1] * 5, labels[i], color = 'b', ha = 'center', va = 'center')\n",
    "plt.xlim(-1,1)\n",
    "plt.ylim(-1,1)\n",
    "plt.xlabel(\"PC{}\".format(1))\n",
    "plt.ylabel(\"PC{}\".format(2))\n",
    "plt.grid()\n",
    "\n",
    "#Call the function. Use only the 2 PCs.\n",
    "myplot(ventral_pcs2[:,0:3],np.transpose(pcav2.components_[0:3, :]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantification of variability within state\n",
    "\n",
    "We want to perform PCA on the ventral state points and identify the direction (eigenvector) that captures a lot of variance but across different trajectories and not just within a single trajectory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ventral_te = time_embedded_copy.copy()\n",
    "\n",
    "pca_te = PCA(n_components=3)\n",
    "ventral_te_pc = pca_te.fit_transform(ventral_te.loc[:,~ventral_te.columns.isin(['state','dataset', 'cluster'])])\n",
    "ventral_te_pc_df = pd.DataFrame(ventral_te_pc)\n",
    "\n",
    "turn_vec = time_embedded_copy['state'].values\n",
    "ventral_te_pc_df['dataset'] = data['dataset'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_names = []\n",
    "intervals = []\n",
    "is_ventral = False\n",
    "count = 0\n",
    "for i in range(ventral_te_pc.shape[0]):\n",
    "    if turn_vec[i] == 'ventral':\n",
    "        if is_ventral:\n",
    "            continue\n",
    "        else:\n",
    "            start_idx=i\n",
    "            is_ventral = True\n",
    "        continue\n",
    "    else:\n",
    "        if not is_ventral:\n",
    "            continue\n",
    "        else:\n",
    "            end_idx=i-1\n",
    "            array = ventral_te_pc_df.loc[start_idx:end_idx,~ventral_te_pc_df.columns.isin(['dataset'])]\n",
    "            dataset_names.append(ventral_te_pc_df['dataset'].loc[start_idx])\n",
    "            intervals.append(array)\n",
    "            is_ventral = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_medians = []\n",
    "for i in intervals:\n",
    "    quan = i.loc[int(np.median(i.index, axis=0)), :]\n",
    "    all_medians.append(quan)\n",
    "    \n",
    "pca_median = PCA(n_components=3)\n",
    "median_pcs = pca_median.fit_transform(pd.DataFrame(all_medians))\n",
    "median_pc1 = median_pcs[:,0]\n",
    "median_comps = pca_median.components_\n",
    "#med = list(np.concatenate(median_pcs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "med_df = pd.DataFrame(median_pc1, columns=['Median'])\n",
    "med_df[\"dataset\"] = dataset_names\n",
    "med_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(data=med_df, x=\"Median\",y=\"dataset\",hue='dataset',bins=250, legend=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(median_pc1, bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vtp = pd.DataFrame(ventral_te_pc)\n",
    "vtp['state'] = turn_vec\n",
    "\n",
    "fig = hf.plot_PCs(vtp)\n",
    "\n",
    "for i in range(len(all_medians)):\n",
    "    fig.add_trace(go.Scatter3d(x=[all_medians[i][0]],\n",
    "                                y=[all_medians[i][1]],\n",
    "                                z=[all_medians[i][2]],\n",
    "                                mode='markers',\n",
    "                                marker=dict(color='black', size=3)))\n",
    "    \n",
    "# scale the components\n",
    "scaled_ventral_components = np.zeros(median_comps.shape)\n",
    "scaled_ventral_components[0]=median_comps[0]*abs_coord_max[0]\n",
    "\n",
    "fig.add_trace(go.Scatter3d(x=[-scaled_ventral_components[0, 0], scaled_ventral_components[0, 0]],\n",
    "                            y=[-scaled_ventral_components[0, 1], scaled_ventral_components[0, 1]],\n",
    "                            z=[-scaled_ventral_components[0, 2], scaled_ventral_components[0, 2]],\n",
    "                            mode='lines', name=f'Principal Component 1',\n",
    "                            line=dict(color='black', width=3)))\n",
    "    \n",
    "fig.update_xaxes(type='linear')\n",
    "fig.update_yaxes(type='linear')\n",
    "fig.update_layout(title='PCA of time-embedded data')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix I: More Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=5)\n",
    "state_clustering = data.copy()\n",
    "state_clustering['cluster'] = kmeans.fit_predict(time_embedded.loc[:,~time_embedded.columns.isin(['state', 'dataset'])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Assuming 'text_column' is your pandas DataFrame column with text values\n",
    "label_encoder = LabelEncoder()\n",
    "state_clustering['dataset_numeric'] = label_encoder.fit_transform(state_clustering['dataset'])\n",
    "\n",
    "labels_pred_proc = state_clustering['cluster']\n",
    "labels_true_proc = state_clustering['dataset_numeric']\n",
    "\n",
    "# Calculate adjusted mutual information score, which tells us how well the clustering results match the ground truth\n",
    "print(adjusted_mutual_info_score(labels_true_proc, labels_pred_proc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
