{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <b>Data Diagnostics I </b> *✲ﾟ*｡✧٩(･ิᴗ･ิ๑)۶*✲ﾟ*｡✧\n",
    "\n",
    "In this notebook we will explore taking the min-max or percentile normalization between datasets and also derivatives and see how our data changes, i.e. the distribution of each variable, check the principal components, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import helper_functions as hf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pynumdiff as pdiff\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from sklearn.covariance import MinCovDet\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from scipy.stats import mstats, boxcox\n",
    "\n",
    "from tqdm import tqdm\n",
    "import importlib\n",
    "importlib.reload(hf)\n",
    "\n",
    "imputed_dataframe = hf.wbstruct_dataframes.loading_pkl('imputed_dataframe.pkl')\n",
    "dataframes = hf.wbstruct_dataframes.loading_pkl('dataframes.pkl')\n",
    "turn_vec = hf.wbstruct_dataframes.loading_pkl('turn_vec.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resampling (or Up-/Downsampling)\n",
    "Our datasets have different sizes, so we have to upsample them. Most recordings range from 3200 to 3780 time points as we can see in the below figure but there is one dataset with 4146 and one with 5450 time points. 8 datasets have exactly 3529 time points. We will therefore down- or upsample to this number via linear interpolation (computing the slope between two data points) implemented in numpy.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf.visualize_fps(dataframes, title=\"frame rate of each dataset\", xlabel=\"dataset\", ylabel=\"frame rate\", coloring=\"tab:red\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resampled_dataframe = imputed_dataframe.copy()\n",
    "\n",
    "frames_num = 3529\n",
    "\n",
    "# length_dict holds the length of each dataset as value and the name of the dataset as key\n",
    "length_dict = defaultdict()\n",
    "for key, value in dataframes.items():\n",
    "    length_dict[key] = len(value)\n",
    "\n",
    "# resample all dataframes to the same length of 3529 frames\n",
    "resampled_dataframe = hf.resample(resampled_dataframe, (length_dict.values()), absolute_frames=frames_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "for key in dataframes.keys():\n",
    "    length_dict[key] = frames_num\n",
    "\n",
    "# we plot all the resampled traces \n",
    "saving_path=\"C:\\\\Users\\\\LAK\\\\Documents\\\\plots\\\\resampled_plots\\\\\"\n",
    "\n",
    "hf.plot_from_stacked_imputed(length_dict, resampled_dataframe, resampled_dataframe, saving_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Truncation\n",
    "\n",
    "We noticed some edge effects in the data, i.e. the first and last 100 time points are not very reliable. We will therefore truncate the data to the middle 3329 time points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths = [3529 for i in range(0, len(dataframes))]\n",
    "frames_num = 3329\n",
    "for key in dataframes.keys():\n",
    "    length_dict[key] = frames_num\n",
    "truncated_dataframe = hf.truncate(resampled_dataframe, lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# we plot all the resampled traces \n",
    "saving_path=\"C:\\\\Users\\\\LAK\\\\Documents\\\\plots\\\\truncated_plots\\\\\"\n",
    "\n",
    "hf.plot_from_stacked_imputed(length_dict, truncated_dataframe, truncated_dataframe, saving_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization between datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen above we have to deal with different scales across datasets so a natural next step is to normalize the data across datasets to make them comparable. We will do this by taking the min-max normalization between datasets. This means that we will take the minimum and maximum value of each variable across all datasets and then normalize each dataset to this range. This will be done the time derivatives of the resampled data.\n",
    "\n",
    "We can also try the percentile normalization between datasets. This means that we will take the 5th and 95th percentile of each variable across all datasets and then normalize each dataset to this range. This will be done the time derivatives of the resampled data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Based on Quantiles: RobustScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = RobustScaler(with_centering=False, with_scaling=True, quantile_range=(5, 95))\n",
    "\n",
    "# normalize per dataset\n",
    "quartiled_separate = hf.normalize_per_dataset(truncated_dataframe, length_dict, scaler)\n",
    "\n",
    "# normalize across datasets \n",
    "quartiled_data = pd.DataFrame(scaler.fit_transform(quartiled_separate), columns = quartiled_separate.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "robust_scaler = RobustScaler(with_centering=True, with_scaling=True, quantile_range=(1, 9))\n",
    "quartiled_data1 = pd.DataFrame(robust_scaler.fit_transform(truncated_dataframe), columns = resampled_dataframe.columns)\n",
    "robust_scaler = RobustScaler(with_centering=True, with_scaling=True, quantile_range=(5, 95))\n",
    "quartiled_data2 = pd.DataFrame(robust_scaler.fit_transform(truncated_dataframe), columns = resampled_dataframe.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import quantile_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting the trace of one neuron across all datasets\n",
    "# and save the plot\n",
    "fig, ax = plt.subplots(figsize=(40, 10))\n",
    "ax.plot(quartiled_data['AVAR'].T, color=\"tab:blue\")\n",
    "ax.set_ylabel(\"AVAR\")\n",
    "ax.set_xlabel(\"time\")\n",
    "ax.set_title(\"AVAR across all datasets\")\n",
    "fig.savefig(\"normalized_AVAR_alldatasets.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%matplotlib widget\n",
    "saving_path=\"C:\\\\Users\\\\LAK\\\\Documents\\\\plots\\\\normalized_plots_10_90\\\\\"\n",
    "\n",
    "hf.plot_from_stacked_imputed(length_dict, quartiled_data, quartiled_data, saving_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%matplotlib widget\n",
    "saving_path=\"C:\\\\Users\\\\LAK\\\\Documents\\\\plots\\\\normalized_plots_10_90_separate\\\\\"\n",
    "\n",
    "hf.plot_from_stacked_imputed(length_dict, quartiled_separate, quartiled_separate, saving_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA on normalized data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "turn_vec =hf.get_behavioural_states(truncated_dataframe) # this step has to be revisited"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "turn_vec.to_pickle(\"turn_vec_truncated.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_quartile = hf.PCA(n_components=3)\n",
    "imputed_pc_quartile = pd.DataFrame(pca_quartile.fit_transform(quartiled_data))\n",
    "\n",
    "window_size = 10\n",
    "\n",
    "# Applying a 10-sample sliding average for smoother visualizations!\n",
    "imputed_pc_quartile[0] = np.convolve(imputed_pc_quartile[0], np.ones(window_size)/window_size, mode='same')\n",
    "imputed_pc_quartile[1] = np.convolve(imputed_pc_quartile[1], np.ones(window_size)/window_size, mode='same')\n",
    "imputed_pc_quartile[2] = np.convolve(imputed_pc_quartile[2], np.ones(window_size)/window_size, mode='same')\n",
    "\n",
    "imputed_pc_quartile['state'] = turn_vec.values\n",
    "hf.plot_PCs(imputed_pc_quartile,imputed_pc_quartile['state'],'PCA_quartiled.html')\n",
    "#hf.plot_PC_gif(imputed_pc_quartile,imputed_pc_quartile['state'],'PCA_quartiled.gif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_quartile = hf.PCA(n_components=3)\n",
    "imputed_pc_quartile = pd.DataFrame(pca_quartile.fit_transform(quartiled_data2))\n",
    "\n",
    "window_size = 10\n",
    "\n",
    "# Applying a 10-sample sliding average for smoother visualizations!\n",
    "imputed_pc_quartile[0] = np.convolve(imputed_pc_quartile[0], np.ones(window_size)/window_size, mode='same')\n",
    "imputed_pc_quartile[1] = np.convolve(imputed_pc_quartile[1], np.ones(window_size)/window_size, mode='same')\n",
    "imputed_pc_quartile[2] = np.convolve(imputed_pc_quartile[2], np.ones(window_size)/window_size, mode='same')\n",
    "\n",
    "imputed_pc_quartile['state'] = turn_vec.values\n",
    "hf.plot_PCs(imputed_pc_quartile,imputed_pc_quartile['state'],'PCA_quartiled.html')\n",
    "#hf.plot_PC_gif(imputed_pc_quartile,imputed_pc_quartile['state'],'PCA_quartiled.gif')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Mahalanobis Distances with normalized data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import mstats, boxcox\n",
    "quartile_copy = quartiled_data.copy()\n",
    "quartile_copy = quartile_copy + abs(quartile_copy.min().min()) + 0.01\n",
    "quartile_transformed = hf.pd.DataFrame()\n",
    "all_lambdas = []\n",
    "for col in quartile_copy.columns:\n",
    "    quartile_transformed[col], best_lambda = boxcox(quartile_copy[col])\n",
    "    all_lambdas.append(best_lambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.covariance import MinCovDet\n",
    "\n",
    "cov = MinCovDet(random_state=0).fit(quartile_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = cov.location_\n",
    "C = cov.covariance_\n",
    "MD2_robust = quartile_transformed.apply(lambda x: np.sqrt((x-t).T @ np.linalg.inv(C) @ (x-t)), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "MD2_robust2 = hf.pd.DataFrame(MD2_robust.copy(), columns=['MD2'])\n",
    "\n",
    "start_index = 0\n",
    "count = 0\n",
    "\n",
    "# we will unstack the dataframe and plot the traces for each dataset\n",
    "for key,obs_count in length_dict.items():\n",
    "\n",
    "    # we take the number of observations from the length dictionary and add it to the start index\n",
    "    end_index = start_index + obs_count\n",
    "    #MD2_truncated = np.concatenate((MD2_truncated[:start_index], MD2_truncated[start_index+100:]))\n",
    "    MD2_robust2.loc[start_index:end_index,'dataset'] = key\n",
    "    start_index = end_index\n",
    "    count += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = hf.px.scatter(y=MD2_robust2[\"MD2\"],x=range(len(MD2_robust2)), title='Robust Mahalanobis Distances on Quartile Normalized Data', labels={'x':'Observation', 'y':'MD'}, color=MD2_robust2[\"dataset\"], color_continuous_scale='viridis')\n",
    "fig.update_traces(marker_size=3)\n",
    "fig.update_layout(legend_title='Dataset')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time Derivatives - Total Variation Regularization\n",
    "Since we are interested in the shape of our data and want to eliminate noise as much as possible, we will take the time derivative of our data. \n",
    "To this end we will use an iterative total variation regularization method to compute the first order derivative of our data. Finite difference methods estimate derivatibes by looking at the changes in the values over small intervals dt. This time step size dt is the reciprocal of the sampling frequency, which is 2.9-3.5 volumes per second for Rebecca's data and about 3 volumes per second for Kerem's data.\n",
    "We will apply this on each dataset individually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resampled_derivatives = hf.compute_derivatives(quartiled_data, length_dict,1,0.01) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resampled_derivatives.to_pickle(\"resampled_derivatives_It1_Gam01.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iteration 2 and 10\n",
    "resampled_derivatives_It2= hf.compute_derivatives(quartiled_data, length_dict, 2, 0.01)\n",
    "resampled_derivatives_It5 = hf.compute_derivatives(quartiled_data, length_dict, 5, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iteration 2 and gamma 0.001 and 10\n",
    "resampled_derivatives_Gam001 = hf.compute_derivatives(quartiled_data, length_dict, 2, 0.001)\n",
    "resampled_derivatives_Gam10 = hf.compute_derivatives(quartiled_data, length_dict, 2, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resampled_derivatives_595_It5_Gam001 = hf.compute_derivatives(quartiled_data2, length_dict, 5, 0.001)\n",
    "resampled_derivatives_595_It5_Gam001.to_pickle(\"resampled_derivatives_595_It5_Gam001.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load from pickle\n",
    "resampled_derivatives_It2 = pd.read_pickle(\"resampled_derivatives_It2.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%matplotlib widget\n",
    "saving_path2=\"C:\\\\Users\\\\LAK\\\\Documents\\\\plots\\\\totalvariation_plots\\\\Iteration2Gamma0.01\\\\\"\n",
    "saving_path10=\"C:\\\\Users\\\\LAK\\\\Documents\\\\plots\\\\totalvariation_plots\\\\Iteration5Gamma0.01\\\\\"\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resampled_derivatives_cumsum = resampled_derivatives.copy()\n",
    "\n",
    "dt = 1/3 # time step: 1/(frame rate)\n",
    "start_index = 0\n",
    "for dataset_idx in tqdm(range(len(dataframes.keys())), desc=\"Computing derivatives\"):\n",
    "    end_index = start_index + frames_num\n",
    "    integrated = np.cumsum(resampled_derivatives_cumsum[start_index:end_index])\n",
    "    resampled_derivatives_cumsum[start_index:end_index] = integrated + abs(integrated.min()) + 0.01 \n",
    "    start_index = end_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# plotting the trace of one neuron across all datasets\n",
    "# and save the plot\n",
    "fig, ax = plt.subplots(figsize=(40, 10))\n",
    "ax.plot(resampled_derivatives_cumsum['AVAR'].T, color=\"tab:blue\")\n",
    "ax.set_ylabel(\"AVAR\")\n",
    "ax.set_xlabel(\"time\")\n",
    "ax.set_title(\"AVAR across all datasets\")\n",
    "fig.savefig(\"resampled_AVAR_alldatasets_It5.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = hf.PCA(n_components=3)\n",
    "temporal_PCs_totalvariation = pd.DataFrame(pca.fit_transform(resampled_derivatives_cumsum))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "window_size = 10\n",
    "\n",
    "# Applyin a 10-sample sliding average for smoother visualizations!\n",
    "temporal_PCs_totalvariation[0] = np.convolve(temporal_PCs_totalvariation[0], np.ones(window_size)/window_size, mode='same')\n",
    "temporal_PCs_totalvariation[1] = np.convolve(temporal_PCs_totalvariation[1], np.ones(window_size)/window_size, mode='same')\n",
    "temporal_PCs_totalvariation[2] = np.convolve(temporal_PCs_totalvariation[2], np.ones(window_size)/window_size, mode='same')\n",
    "\n",
    "temporal_PCs_totalvariation['state'] = turn_vec.values\n",
    "hf.plot_PCs(temporal_PCs_totalvariation,temporal_PCs_totalvariation['state'],'PCA_derivatives_totalvariation.html')\n",
    "hf.plot_PC_gif(temporal_PCs_totalvariation,temporal_PCs_totalvariation['state'],'PCA_totalvariation.gif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = hf.PCA(n_components=3)\n",
    "temporal_PCs_totalvariation = pd.DataFrame(pca.fit_transform(resampled_derivatives_It2_cumsum))\n",
    "\n",
    "# Applyin a 10-sample sliding average for smoother visualizations!\n",
    "temporal_PCs_totalvariation[0] = np.convolve(temporal_PCs_totalvariation[0], np.ones(window_size)/window_size, mode='same')\n",
    "temporal_PCs_totalvariation[1] = np.convolve(temporal_PCs_totalvariation[1], np.ones(window_size)/window_size, mode='same')\n",
    "temporal_PCs_totalvariation[2] = np.convolve(temporal_PCs_totalvariation[2], np.ones(window_size)/window_size, mode='same')\n",
    "\n",
    "temporal_PCs_totalvariation['state'] = turn_vec.values\n",
    "hf.plot_PCs(temporal_PCs_totalvariation,temporal_PCs_totalvariation['state'],'PCA_derivatives_totalvariation_It2.html')\n",
    "hf.plot_PC_gif(temporal_PCs_totalvariation,temporal_PCs_totalvariation['state'],'PCA_totalvariation_It2.gif')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Butterworth Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "butterworth_derivatives = quartiled_data.copy()\n",
    "dt = 1/3 # time step: 1/(frame rate)\n",
    "start_index = 0\n",
    "for dataset in dataframes.values():\n",
    "    end_index = start_index + frames_num\n",
    "    for col_index in range(len(butterworth_derivatives.columns)):\n",
    "        x_hat, dxdt_hat = pdiff.smooth_finite_difference.butterdiff(resampled_derivatives.iloc[start_index:end_index, col_index], dt, [3, 0.09], options={'iterate': False}) # x_hat: estimated (smoothed) x, dxdt_hat: estimated dx/dt, [1, 0.0001]: regularization parameters -> gamma=0.2 is too high, derivatives become too blocky\n",
    "        butterworth_derivatives.iloc[start_index:end_index, col_index] = dxdt_hat\n",
    "    #if end_index != len(resampled_derivatives):\n",
    "    #    resampled_derivatives.iloc[end_index, :] = np.nan #so that we have a separation between datasets   \n",
    "    start_index = end_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%matplotlib widget\n",
    "saving_path=\"C:\\\\Users\\\\LAK\\\\Documents\\\\butterworth_plots\\\\\"\n",
    "\n",
    "\n",
    "start_index = 0\n",
    "count = 0\n",
    "\n",
    "# we will unstack the dataframe and plot the traces for each dataset\n",
    "for obs_count in list(length_dict.values()):\n",
    "\n",
    "    # we take the number of observations from the length dictionary and add it to the start index\n",
    "    end_index = start_index + obs_count\n",
    "    res_data_df = butterworth_derivatives.iloc[start_index:end_index]\n",
    "\n",
    "    fig = hf.plot_traces.make_grid_plot_from_two_dataframes(\n",
    "            res_data_df, res_data_df)\n",
    "    # fig, ax = plot_traces.make_grid_plot_from_dataframe(df_imputed)\n",
    "\n",
    "    # save all plots in a folder\n",
    "    pathname = saving_path + list(length_dict.keys())[count] + \".png\"\n",
    "    fig.savefig(pathname)\n",
    "    plt.close(fig)\n",
    "    start_index = end_index\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resampled_derivatives_butter_cumsum = resampled_derivatives.copy()\n",
    "\n",
    "dt = 1/3 # time step: 1/(frame rate)\n",
    "start_index = 0\n",
    "for dataset_idx in tqdm(range(len(dataframes.keys())), desc=\"Computing derivatives\"):\n",
    "    end_index = start_index + pts\n",
    "    integrated_bt = np.cumsum(butterworth_derivatives[start_index:end_index])\n",
    "    resampled_derivatives_butter_cumsum[start_index:end_index] = integrated_bt + abs(integrated_bt.min()) + 0.01\n",
    "\n",
    "    start_index = end_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = hf.PCA(n_components=3)\n",
    "pca_butterworth = pd.DataFrame(pca.fit_transform(resampled_derivatives_butter_cumsum))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg = pca_butterworth#.iloc[68595:72344]\n",
    "avg[\"state\"] = turn_vec.values#iloc[68595:72344].values\n",
    "\n",
    "avg[0] = np.convolve(avg[0], np.ones(window_size)/window_size, mode='same')\n",
    "avg[1] = np.convolve(avg[1], np.ones(window_size)/window_size, mode='same')\n",
    "avg[2] = np.convolve(avg[2], np.ones(window_size)/window_size, mode='same')\n",
    "\n",
    "hf.plot_PCs(avg,avg[\"state\"] ,'PCA_butterworth.html')\n",
    "hf.plot_PC_gif(avg,avg[\"state\"] ,'PCA_butterworth.gif')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
