{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc6f0068-5222-4cdd-82bc-c2d5c0751466",
   "metadata": {},
   "source": [
    "# Convert immobilized to nwb\n",
    "\n",
    "For an initial analyzing task to play around with different algorithms, we will be using immobilized data from former coworkers Kerem and Rebecca. They used the [whole brain analyzer ](https://github.com/Zimmer-lab/whole_brain_analyzer_MATLAB) to preprocess the raw data which includes:\n",
    "\n",
    "- normalizing (delta f over f)\n",
    "- bleach correction\n",
    "- bg correction (if shining from neighbouring neurons disturb the signal)\n",
    "\n",
    "We should check the bleach correction version of the data to check if further preprocessing is needed, in case some artifacts remain.\n",
    "\n",
    "Unfortunately, in Rebecca's data the ID columns are empty.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d8a2764-c12f-48c7-a1a2-e7095b44738c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'helper' from 'c:\\\\Users\\\\LAK\\\\Documents\\\\helper.py'>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import helper as helper\n",
    "import dill\n",
    "import importlib\n",
    "\n",
    "# in case we want to reload our module\n",
    "#importlib.reload(helper)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9584c141-073b-489e-8ddb-77a69845ddd8",
   "metadata": {},
   "source": [
    "### Get deltaFOverF Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1c861f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining directory paths where all recordings are located, first is from Rebecca and second from Kerem Uzel\n",
    "directory_paths = [\"Y:\\\\lisc\\\\project\\\\neurobiology\\\\zimmer\\\\Rebecca\",\n",
    "                   \"Y:\\\\lisc\\\\project\\\\neurobiology\\\\zimmer\\\\Kerem_Uzel\\\\Whole_brain_imaging\\\\Cleaned_up_datasets\\\\WT\"]\n",
    "\n",
    "# defining target file name which should be wbstruct.mat since this is the file from the wba that we want to convert into a DF\n",
    "target_file = \"wbstruct.mat\"\n",
    "\n",
    "# since we usually have multiple recordings per animal we want to include only those that are relevant for our analysis\n",
    "# following is specific to Rebecca's and Kerem's Data\n",
    "include_Rebecca=[\"Ctrl\",\"used Datasets\"]\n",
    "include_Kerem=[\"Head\",\"Tail\"]\n",
    "exclude=[\"not_used\",\"not used\",\"notUsed\",\"cat-2_tdc-1_tph-1\"]\n",
    "recording_type=\"deltaFOverF_bc\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3c5a1a63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for paths\n",
      "Found 56 paths\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Files: 100%|██████████| 56/56 [01:50<00:00,  1.97s/it]\n"
     ]
    }
   ],
   "source": [
    "# import and save rebecca's data\n",
    "datasets_rebecca = helper.get_datasets_dict(directory_paths[0],target_file,include_Rebecca,exclude,recording_type) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6c00c9e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for paths\n",
      "Found 12 paths\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Files: 100%|██████████| 12/12 [29:18<00:00, 146.51s/it]\n"
     ]
    }
   ],
   "source": [
    "# import and save kerem's data\n",
    "datasets_kerem = helper.get_datasets_dict(directory_paths[1],target_file,include_Kerem,exclude,recording_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2672e84c-42f4-4292-a1bc-28a691a24342",
   "metadata": {},
   "source": [
    "## Specific to Rebecca's Data\n",
    "### Get IDs from Excel Sheets\n",
    "Following is specific to Rebecca's data (see the directory path below) where the IDs are stored in separate Excel Sheets. In the get_IDs_dict, we iterate through Excel sheets and if we have columns associated with the ID and the index then we will collect it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e22b9b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we want to import the excel files that contain the information about the recordings\n",
    "directory_path_ID = \"Y:\\\\lisc\\\\project\\\\neurobiology\\\\zimmer\\\\Rebecca\\\\Analyses\"\n",
    "target_ID = \".xlsx\"\n",
    "include_ID = [\"Analyses_\"]\n",
    "exclude_ID = [\"._\",\"cat-2_tdc-1_tph-1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "18258a86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for paths\n",
      "Found 3 paths\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Files: 100%|██████████| 3/3 [00:03<00:00,  1.07s/it]\n"
     ]
    }
   ],
   "source": [
    "dictofIDs_og = helper.get_IDs_dict(directory_path_ID, target_ID, include_ID, exclude_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1734bf0f",
   "metadata": {},
   "source": [
    "### Merging Head and Tail of Rebecca's Data and converting to DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7525e512-ef58-42d9-b06e-cdba929c9aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_rebecca_copy = copy.deepcopy(datasets_rebecca)\n",
    "dictofIDs = copy.deepcopy(dictofIDs_og)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "52410a70-4925-4d65-be17-229a756edca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for trial,trialvalue in datasets_rebecca.items():\n",
    "    \n",
    "    if \"notUsed\" not in trial:\n",
    "        \n",
    "        id_names = dictofIDs[trial]\n",
    "\n",
    "        # merging head and tail data if both are available\n",
    "        try:\n",
    "            merged_datasets=np.hstack((datasets_rebecca[trial][\"Head\"]['deltaFOverF'],datasets_rebecca[trial][\"Tail\"]['deltaFOverF']))\n",
    "        except ValueError as e:\n",
    "            merged_datasets=np.hstack((datasets_rebecca[trial][\"Head\"]['deltaFOverF']))\n",
    "\n",
    "        id_length = merged_datasets.shape[1]\n",
    "\n",
    "        # if the number of neurons in the recording is not equal to the number of IDs we want to exclude this recording\n",
    "        if not len(id_names)==id_length:\n",
    "            if not len(id_names)==(id_length-datasets_rebecca[trial][\"Head\"]['deltaFOverF'].shape[1]):\n",
    "                del datasets_rebecca_copy[trial]\n",
    "                continue\n",
    "            else:\n",
    "                id_length=id_length-datasets_rebecca[trial][\"Head\"]['deltaFOverF'].shape[1]\n",
    "                merged_datasets=datasets_rebecca[trial][\"Tail\"]['deltaFOverF']\n",
    "\n",
    "        # creating column names for the DF from the IDs that we got\n",
    "        colnames = [f\"neuron_{i:03d}\" for i in range(id_length)]\n",
    "        colnames = [dummy if pd.isna(ID) else ID for dummy, ID in zip(colnames, id_names)]\n",
    "        datasets_rebecca_copy[trial] = pd.DataFrame(merged_datasets, columns=colnames) \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3429f3e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Recordings: ['20200629_w1', '20200708_w3', '20200714_w3', '20200716_w2', '20200724_w3', '20200724_w4', '20200826_w3', '20200826_w6', '20200922_w1', '20200930_w4', '20201007_w4', '20210112_w3', '20210113_w1', '20210126_w2', '20210203_w2', '20210323_w6', '20210324_w5', '20210330_w5', '20210414_w7']\n"
     ]
    }
   ],
   "source": [
    "print(\"Available Recordings:\",list(datasets_rebecca_copy.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade514f0",
   "metadata": {},
   "source": [
    "## Merging Kerem's Head and Tail\n",
    "Following is specific to Kerem's data. In this data case we have the IDs already stored in the datasets dictionary, so the merge should be easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "12fefe38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load kerem's dictionary\n",
    "with open('datasets_kerem_dFOF_bc.pkl' ,'rb') as f:\n",
    "    datasets_kerem = pickle.load(f)\n",
    "\n",
    "datasets_kerem_copy = copy.deepcopy(datasets_kerem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ae4d26ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "for trial,trialvalue in datasets_kerem.items():\n",
    "    \n",
    "    \n",
    "    id_names = datasets_kerem[trial][\"Head\"][\"ID1\"]+datasets_kerem[trial][\"Tail\"][\"ID1\"]\n",
    "\n",
    "    # merging head and tail data\n",
    "    merged_datasets=np.hstack((datasets_kerem[trial][\"Head\"][recording_type],datasets_kerem[trial][\"Tail\"][recording_type]))\n",
    "    id_length = merged_datasets.shape[1]\n",
    "    colnames = [f\"neuron_{i:03d}\" for i in range(id_length)]\n",
    "    colnames = [dummy if pd.isna(ID) else ID for dummy, ID in zip(colnames, id_names)]\n",
    "    datasets_kerem_copy[trial] = pd.DataFrame(merged_datasets, columns=colnames) \n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d2a555c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Recordings: ['Dataset1_20190125_ZIM1428_Ctrl_w2', 'Dataset2_20180207_TKU862_Ctrl_w6', 'Dataset3_20180112_TKU761_Ctrl_w1', 'Dataset4_20170927_ZIM1428_Ctrl_w7', 'Dataset5_20181217_ZIM1428_w2', 'Dataset6_20190315_ZIM1428_1xHis_w1']\n"
     ]
    }
   ],
   "source": [
    "print(\"Available Recordings:\",list(datasets_kerem_copy.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e38c367",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "e6b8062c8d873bf17e80439cf9ee27761532659cc2634ffd2b6294fa54795af1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
